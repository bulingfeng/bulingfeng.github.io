---
title: "Agent,Function Calling,Mcp都是什么？"
subtitle: "Agent,Function Calling,Mcp都是什么？"
layout: post
author: "bulingfeng"
header-style: text
tags:
- AI
---
# 关于AI当中的一些专有名词

## Prompt

Prompt 分为两种：

> - User Prompt
> - System Prompt

由于 AI 需要一个人设，有一个人设之后它才能更准确的回答自己的问题，所以就需要自己使用 user prompt 来给 AI 制定人设。比如：

> 你是一个高级 java 开发工程师，你可以帮助我编写各种程序，其中包括表结构的建立...

如果我们不想做这么一个多余的事情，我想要一开始就让 AI 有个人设，那么就直接设置一个 system prompt 即可。

System prompt 其实比 user prompt 更加的合理，因为每次当用户发送消息的时候，回把 AI 的上下文+System Prompt 也给发送给 AI，这样就能够减少大模型出现的幻觉。

缺点：

> 虽然大模型有个一个人设，但是它还是一个聊天机器人，具体的工作还的自己来完成。

## AI Agent

agent 其实说简单点就是能够按照用户的需求来执行制定内容生成的大模型人设。

比如自己写了两个 python 的函数，一个是创建文件，一个是删除文件。这个时候就需要把这两个函数入参和功能都统一发送给大模型，然后 AI 大模型就可以根据自然语言来进行对应的操作。

比如我说：帮我在 c 盘创建一个 a.txt 文件，这个时候 ai 就可以给个创建文件的路径和文件名字，这个时候再调用函数来进行执行了。

其中执行大模型的这块东西，可以认为是一个 agent，而创建或者删除文件的函数，可以操作 AI TOOL。但是这样的架构有个问题：

> 大模型是一个概率模型，它并不能够百分之百按照你想要的格式进行返回数据。
>
> 解决方案：
>
> > 不断的和大模型进行交互重试，直到返回的格式符合函数的要求。（很费 token）

作用

> 1. 维护上下文（保持大模型有记忆）
> 2. 含有要执行 tool 的一些元数据信息

## Function Calling

function calling 会用一个 json 对象来约束调用函数的信息和返回信息的格式。

```python
functions = [
    {
        "name": "create_file",
        "description": "Create a text file with given name and content",
        "parameters": {
            "type": "object",
            "properties": {
                "filename": {
                    "type": "string",
                    "description": "The name of the file to create"
                },
                "content": {
                    "type": "string",
                    "description": "The content to write into the file"
                }
            },
            "required": ["filename", "content"]
        }
    }
]

# 第一步：发送用户消息 + 函数定义
user_message = "帮我创建一个文件，名字是 demo.txt，内容是 Hello AI World!"

response = client.chat.completions.create(
    model="gpt-4.1",  # 或 gpt-5
    messages=[{"role": "user", "content": user_message}],
    functions=functions
)

# 检查模型是否想调用函数
message = response.choices[0].message

if message.get("function_call"):
    func_name = message.function_call.name
    args = json.loads(message.function_call.arguments)
    print(f"模型请求调用函数: {func_name}({args})")

    # 实际执行函数逻辑
    if func_name == "create_file":
        filename = args["filename"]
        content = args["content"]
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"✅ 已创建文件 {filename}")

        # 再次将函数结果返回给模型
        second_response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "user", "content": user_message},
                message,  # 模型的 function_call
                {"role": "function", "name": func_name, "content": "File created successfully"}
            ]
        )
       print(second_response.choices[0].message.content)
```

Function calling的缺点：

> 每个大模型厂商的function calling的格式并不统一，这就造成一个尴尬的局面，要对各个厂商都写一套自己的function calling。

## MCP

在没MCP之前，每个agent其实都是程序员手撸代码的效果，通过大模型把参数给反回来，然后再解析函数给自己写好的函数，从而完成了逻辑实现。

但是很多函数是可复用性的，跨公司和跨部门都copy一套代码显然不合适，于是MCP就诞生了。

也就是把一个MCP给服务化了,并且规定了client端和server端交互的统一标准。

> 1. 执行tool的叫做mcp-server
> 2. 调用mcp-server的叫做 mcp-client

简单来说：

当用户把提出来一个问题，比如今天的天气会下雨吗？

mcp-client会把这个用户提示词发送给 mcp -server,这个时候 mcp 转换成 function-calling 的格式（包括各种 tool 的信息）发送给大模型，然后大模型会发现一个查询天气的接口，然后把天气接口相对应的参数按照要求给反回来，然后查询天气的接口，传入参数，把天气的结果数据，发送给大模型，最后大模型返回符合人类易读的信息返回给前端。